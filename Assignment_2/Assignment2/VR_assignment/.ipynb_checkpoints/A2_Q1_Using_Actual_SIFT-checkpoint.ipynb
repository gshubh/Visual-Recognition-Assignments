{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = cv2.imread(\"institute1.jpg\",1)           # FIRST PART OF INSTITUTE\n",
    "copy_img1 = cv2.imread(\"institute1.jpg\",1)\n",
    "img2 = cv2.imread(\"institute2.jpg\",1)           # SECOND PART OF THE INSTITUTE\n",
    "copy_img_2 = cv2.imread(\"institute2.jpg\",1)\n",
    "img3 = cv2.imread(\"secondPic1.jpg\",1)           # FIRST PART OF OUTSIDE LIBRARY\n",
    "copy_img3 = cv2.imread(\"secondPic1.jpg\",1)\n",
    "img4 = cv2.imread(\"secondPic2.jpg\",1)           # SECOND PART OF OUTSIDE LIBRARY\n",
    "copy_img4 = cv2.imread(\"secondPic2.jpg\",1)\n",
    "\n",
    "# \"sift\" function is used to find the key point and descriptor of image,like nose, eyes, lips, eyebros etc in face image \n",
    "sift = cv2.xfeatures2d.SIFT_create()    \n",
    "\n",
    "keypoints1,descriptors1 = sift.detectAndCompute(img1,None)\n",
    "keypoints2,descriptors2 = sift.detectAndCompute(img2,None)\n",
    "keypoints3,descriptors3 = sift.detectAndCompute(img3,None)\n",
    "keypoints4,descriptors4 = sift.detectAndCompute(img4,None)\n",
    "\n",
    "\n",
    "# Image that contain key points in the First part of Institute. \n",
    "img5 = cv2.drawKeypoints(copy_img1,keypoints1,copy_img1)\n",
    "cv2.imshow(\"KPOI\",img5)      \n",
    "cv2.imwrite(\"KPOI1.jpg\",img5)\n",
    "\n",
    "# Image that contain key points in the Second part of Institute.\n",
    "img6 = cv2.drawKeypoints(copy_img_2,keypoints2,copy_img_2)\n",
    "cv2.imshow(\"KPOI\",img6)\n",
    "cv2.imwrite(\"KPOI2.jpg\",img6)\n",
    "\n",
    "# Image that contain key points in the Second part of Outside Library.\n",
    "img7 = cv2.drawKeypoints(copy_img3,keypoints3,copy_img3)\n",
    "cv2.imshow(\"KPOI\",img7)\n",
    "cv2.imwrite(\"KPOL1.jpg\",img7)\n",
    "\n",
    "# Image that contain key points in the Second part of Outside Library.\n",
    "img8 = cv2.drawKeypoints(copy_img4,keypoints4,copy_img4)\n",
    "cv2.imshow(\"KPOI\",img8)\n",
    "cv2.imwrite(\"KPOL2.jpg\",img8)\n",
    "\n",
    "\n",
    "bruteforcematcher = cv2.BFMatcher(cv2.NORM_L2,crossCheck=True)    # Match the descriptor of two images and check the common part in two images\n",
    "\n",
    "imagematch1 = bruteforcematcher.match(descriptors1,descriptors2)  # match the descriptor of two images of the institute.\n",
    "imagematch2 = bruteforcematcher.match(descriptors3,descriptors4)  # match the descriptor of two images of the outside library.\n",
    "imagematch3 = sorted(imagematch1,key=lambda x:x.distance)         # Sorting of the descriptors on the basis of distance \n",
    "imagematch4 = sorted(imagematch2,key=lambda x:x.distance)         # Sorting of the descriptors on the basis of distance\n",
    "\n",
    "match_img1 = cv2.drawMatches(img1,keypoints1,img2,keypoints2,imagematch3[:150],None,flags=2)  # Join the match points of image 1 and image 2 by a straight line\n",
    "match_img2 = cv2.drawMatches(img3,keypoints3,img4,keypoints4,imagematch4[:150],None,flags=2)  # Join the match points of image 3 and image 4 by a straight line\n",
    "\n",
    "\n",
    "cv2.imshow(\"INSTITUTE IMAGE AFTER MATCHING POINTS\",match_img1)\n",
    "cv2.imwrite(\"match_img_1.jpg\",match_img1)\n",
    "cv2.imshow(\"OUTSIDE LIBRARY IMAGE AFTER MATCHING POINTS\",match_img2)\n",
    "cv2.imwrite(\"match_img_2.jpg\",match_img2)\n",
    "\n",
    "src_pts1 = np.float32([keypoints1[m.queryIdx].pt for m in imagematch3]).reshape(-1,1,2) # fill the common point of both images in array\n",
    "des_pts1 = np.float32([keypoints2[m.trainIdx].pt for m in imagematch3]).reshape(-1,1,2)\n",
    "src_pts2 = np.float32([keypoints3[m.queryIdx].pt for m in imagematch4]).reshape(-1,1,2)\n",
    "des_pts2 = np.float32([keypoints4[m.trainIdx].pt for m in imagematch4]).reshape(-1,1,2)\n",
    "\n",
    "matrix1,_=cv2.findHomography(src_pts1,des_pts1,cv2.RANSAC,5.0)  # findHomography function give the transformation matrix of transformation bw src_pts1 and des_pts1.\n",
    "matrix2,_=cv2.findHomography(src_pts2,des_pts2,cv2.RANSAC,5.0)  # findHomography function give the transformation matrix of transformation bw src_pts2 and des_pts2.\n",
    "\n",
    "\n",
    "# To apply perspective transformation to both images, \"warpPerspective\" transforms the source image using the specified matrix.\n",
    "img9 = cv2.warpPerspective(img2,matrix1,(img1.shape[1]+img2.shape[1],img1.shape[0]))   # img9 is WARPED IMAGE OF INSTITUTE.\n",
    "img10 = cv2.warpPerspective(img4,matrix2,(img3.shape[1]+img4.shape[1],img3.shape[0]))  # img10 is WARPED IMAGE OF OUTSIDE LIBRARY.\n",
    "\n",
    "\n",
    "copy_img1 = np.concatenate([img1,img9],axis=1)  # Concatenation of 1st image of institute(img1) and WARPED IMAGE OF INSTITUTE.\n",
    "img12 = np.concatenate([img3,img10],axis=1) # Concatenation of 1st image of outside library(img3) and WARPED IMAGE OF OUTSIDE LIBRARY.\n",
    "\n",
    "cv2.imshow(\"PANORAMA OF INSTITUTE\",copy_img1)\n",
    "cv2.imwrite(\"pano1.jpg\",copy_img1)\n",
    "cv2.imshow(\"PANORAMA OF OUTSIDE LIBRARY\",img12)\n",
    "cv2.imwrite(\"pano2.jpg\",img12)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
